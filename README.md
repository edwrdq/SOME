SOME: Sparse Organic Modular Experts

A lightweight experimental MoE-style architecture with emergent specialization, built in JAX.

SOME is an architecture designed to explore organic specialization in sparse models.
Instead of enforcing balance or rigid expert routing, SOME allows experts to self-organize over training, forming roles naturally through gradient pressure and task structure.

This repo contains:
	‚Ä¢	A JAX/Flax implementation of the SOME block
	‚Ä¢	A tiny transformer using SOME as its feedforward module
	‚Ä¢	Training utilities
	‚Ä¢	A Palette-based TUI for visualizing expert behavior
	‚Ä¢	Early toy-task experiments showing specialization
	‚Ä¢	A growing framework for LM training and benchmarking

‚∏ª

‚ú® Key Ideas

1. Organic Specialization

Experts are not assigned fixed roles.
The router selects experts based on token-level context, and over time experts self-organize into functional groups (syntax, math, structure, etc.).

2. Sparse Activation

Only a subset of experts run per token, giving:
	‚Ä¢	higher effective capacity
	‚Ä¢	lower compute cost
	‚Ä¢	reduced interference between subskills

3. Simple & Interpretable Routing

The router is a small MLP that outputs logits over experts.
Top-k routing (or threshold routing) picks the active experts.
No balancing loss is used by default ‚Äî specialization emerges naturally.

4. Lightweight Research Platform

The codebase is intentionally small and hackable:
	‚Ä¢	easy to modify routing
	‚Ä¢	easy to add experts
	‚Ä¢	easy to scale depth/width
	‚Ä¢	ideal for experimentation on a single GPU

üß† Roadmap

Phase 1 (done / in progress)
	‚Ä¢	Implement SOME block
	‚Ä¢	Build routing mechanism
	‚Ä¢	Toy dataset specialization
	‚Ä¢	TUI visualization

Phase 2
	‚Ä¢	Add tokenizer (GPT2 or SentencePiece)
	‚Ä¢	Train tiny language models (5M‚Äì20M params)
	‚Ä¢	Observe expert specialization on text
	‚Ä¢	Add dataset loaders (WikiText, small Wikipedia subsets)

Phase 3
	‚Ä¢	Scale to 150M parameters
	‚Ä¢	Train on mixed text + code
	‚Ä¢	Evaluate on:
	‚Ä¢	HLE (Humanity‚Äôs Last Exam)
	‚Ä¢	ARC-AGI (partial)
	‚Ä¢	HumanEval / MBPP
	‚Ä¢	AI ‚ÄúIQ‚Äù pattern tests

Phase 4
	‚Ä¢	Add tool use (MCP)
	‚Ä¢	Extend architecture (hierarchical experts, gated clusters)
	‚Ä¢	Write the research paper
	‚Ä¢	Release models + benchmarks

‚∏ª

üìä Goals

SOME is not built to beat trillion-dollar labs.
Instead, its goals are:
	‚Ä¢	Show emergent specialization in a tiny sparse model
	‚Ä¢	Match or beat dense models 2‚Äì5√ó larger on structured tasks
	‚Ä¢	Provide a transparent research platform for sparse routing
	‚Ä¢	Inspire further exploration of decentralized expert behavior

‚∏ª

üìù License

Choose whatever you prefer (MIT, Apache-2.0, GPL, etc.).

‚∏ª

üë• Contributions

Open to:
	‚Ä¢	researchers
	‚Ä¢	engineers
	‚Ä¢	students
	‚Ä¢	people interested in sparse models

PRs welcome once the repo stabilizes.

‚∏ª

üôè Acknowledgments

Inspired by:
	‚Ä¢	Mixture-of-Experts literature
	‚Ä¢	GShard, Switch Transformer, Mixtral
	‚Ä¢	Sparse architectures and routing research

But SOME is intentionally not a direct copy of any of them.
